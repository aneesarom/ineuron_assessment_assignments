{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c92a11",
   "metadata": {},
   "source": [
    "# Q-1. \n",
    "```\n",
    "Imagine you have a dataset where you have different Instagram features\n",
    "like u sername , Caption , Hashtag , Followers , Time_Since_posted , and likes , now your task is\n",
    "to predict the number of likes and Time Since posted and the rest of the features are\n",
    "your input features. Now you have to build a model which can predict the\n",
    "number of likes and Time Since posted.\n",
    "Dataset This is the Dataset You can use this dataset for this question.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e37de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15192526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"instagram_reach (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9cbf4a19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S.No</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Time since posted</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>michaelgarza__</td>\n",
       "      <td>328 S. Wetherly Drive, Beverly Hills, CA 90212...</td>\n",
       "      <td>614</td>\n",
       "      <td>#beverlyhills #realestate#losangelesrealestate...</td>\n",
       "      <td>3 hours</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>dvlp_search</td>\n",
       "      <td>Credit @tristankappel To find more dvlp follow...</td>\n",
       "      <td>450</td>\n",
       "      <td>#workspace #work #developer#development #devel...</td>\n",
       "      <td>3 hours</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>ecom.space</td>\n",
       "      <td>We are coming up with the Best 21 Books that w...</td>\n",
       "      <td>182</td>\n",
       "      <td>#books #book #motivation #inspiration #life#bo...</td>\n",
       "      <td>3 hours</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>lb3enterprises</td>\n",
       "      <td>We’re only paid to move dirt once. It’s not ju...</td>\n",
       "      <td>2039</td>\n",
       "      <td>#heavyequipment #underconstruction#dozer #real...</td>\n",
       "      <td>3 hours</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>palmariusdev</td>\n",
       "      <td>Obtén tu tienda en línea ahora.</td>\n",
       "      <td>741</td>\n",
       "      <td>#marketing #programming#development #desarroll...</td>\n",
       "      <td>3 hours</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  S.No        USERNAME  \\\n",
       "95           8    19  michaelgarza__   \n",
       "96           9    21     dvlp_search   \n",
       "97          10    22      ecom.space   \n",
       "98          11    24  lb3enterprises   \n",
       "99          12    25    palmariusdev   \n",
       "\n",
       "                                              Caption  Followers  \\\n",
       "95  328 S. Wetherly Drive, Beverly Hills, CA 90212...        614   \n",
       "96  Credit @tristankappel To find more dvlp follow...        450   \n",
       "97  We are coming up with the Best 21 Books that w...        182   \n",
       "98  We’re only paid to move dirt once. It’s not ju...       2039   \n",
       "99                    Obtén tu tienda en línea ahora.        741   \n",
       "\n",
       "                                             Hashtags Time since posted  Likes  \n",
       "95  #beverlyhills #realestate#losangelesrealestate...           3 hours     31  \n",
       "96  #workspace #work #developer#development #devel...           3 hours     42  \n",
       "97  #books #book #motivation #inspiration #life#bo...           3 hours     10  \n",
       "98  #heavyequipment #underconstruction#dozer #real...           3 hours    222  \n",
       "99  #marketing #programming#development #desarroll...           3 hours    109  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d54e20bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"USERNAME\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adb2ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Time since posted\"] = df[\"Time since posted\"].apply(lambda x: x.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "269b64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df[\"Caption\"] = df[\"Caption\"].replace(np.nan, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c1b9480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Hashtags\"] = df[\"Hashtags\"].apply(lambda x: x.replace(\"#\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c35d7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Time since posted\"] = df['Time since posted'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ebaa1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"caption_length\"] = df[\"Caption\"].apply(lambda x: len(x.split(\" \")))\n",
    "df[\"hashtag_length\"] = df[\"Hashtags\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e518a5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S.No</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Time since posted</th>\n",
       "      <th>Likes</th>\n",
       "      <th>caption_length</th>\n",
       "      <th>hashtag_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869163</td>\n",
       "      <td>0.208473</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>0.181740</td>\n",
       "      <td>0.174075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No</th>\n",
       "      <td>0.869163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.172002</td>\n",
       "      <td>-0.110024</td>\n",
       "      <td>0.009721</td>\n",
       "      <td>0.100464</td>\n",
       "      <td>0.118723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Followers</th>\n",
       "      <td>0.208473</td>\n",
       "      <td>0.172002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.251154</td>\n",
       "      <td>0.266218</td>\n",
       "      <td>-0.035585</td>\n",
       "      <td>0.000876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time since posted</th>\n",
       "      <td>0.017372</td>\n",
       "      <td>-0.110024</td>\n",
       "      <td>0.251154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.609888</td>\n",
       "      <td>-0.017097</td>\n",
       "      <td>-0.095140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Likes</th>\n",
       "      <td>0.010271</td>\n",
       "      <td>0.009721</td>\n",
       "      <td>0.266218</td>\n",
       "      <td>0.609888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018454</td>\n",
       "      <td>-0.141778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caption_length</th>\n",
       "      <td>0.181740</td>\n",
       "      <td>0.100464</td>\n",
       "      <td>-0.035585</td>\n",
       "      <td>-0.017097</td>\n",
       "      <td>-0.018454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashtag_length</th>\n",
       "      <td>0.174075</td>\n",
       "      <td>0.118723</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>-0.095140</td>\n",
       "      <td>-0.141778</td>\n",
       "      <td>0.194422</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Unnamed: 0      S.No  Followers  Time since posted  \\\n",
       "Unnamed: 0           1.000000  0.869163   0.208473           0.017372   \n",
       "S.No                 0.869163  1.000000   0.172002          -0.110024   \n",
       "Followers            0.208473  0.172002   1.000000           0.251154   \n",
       "Time since posted    0.017372 -0.110024   0.251154           1.000000   \n",
       "Likes                0.010271  0.009721   0.266218           0.609888   \n",
       "caption_length       0.181740  0.100464  -0.035585          -0.017097   \n",
       "hashtag_length       0.174075  0.118723   0.000876          -0.095140   \n",
       "\n",
       "                      Likes  caption_length  hashtag_length  \n",
       "Unnamed: 0         0.010271        0.181740        0.174075  \n",
       "S.No               0.009721        0.100464        0.118723  \n",
       "Followers          0.266218       -0.035585        0.000876  \n",
       "Time since posted  0.609888       -0.017097       -0.095140  \n",
       "Likes              1.000000       -0.018454       -0.141778  \n",
       "caption_length    -0.018454        1.000000        0.194422  \n",
       "hashtag_length    -0.141778        0.194422        1.000000  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8ce3dcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  2,  3,  4,  7,  8,  9,  5, 20, 14, 24])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Time since posted\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "434ccd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  # To handle stopwords dictionary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string  # to handle punctuations and special characters\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4328bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f43d0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map the part of speech tags from NLTK to WordNet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNet lemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def message_text_process(mess):\n",
    "    mess = remove_word_with_digit_and_non_ascii(mess)\n",
    "    # remove punctuation from string\n",
    "    no_punctuation = [char for char in mess if char not in string.punctuation] \n",
    "    no_punctuation = ''.join(no_punctuation) # joining all the characters without punctuation\n",
    "    # remove stop words\n",
    "    words = [word.lower() for word in no_punctuation.split() if word not in stopwords.words('english')]\n",
    "    words = \" \".join(words)\n",
    "    # lemmatize the text\n",
    "    tokens = nltk.word_tokenize(words)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, pos=get_wordnet_pos(token))\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def remove_word_with_digit_and_non_ascii(sentence):\n",
    "    final_sentence = \"\"\n",
    "    sentence = ''.join(c for c in sentence if ord(c) < 128)\n",
    "    for word in sentence:\n",
    "        if not any(char.isdigit() for char in word):\n",
    "            final_sentence += word\n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1bda4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Caption\"] = df[\"Caption\"].apply(message_text_process)\n",
    "df[\"Hashtags\"] = df[\"Hashtags\"].apply(message_text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "77e624a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"common_words\"] = df.apply(lambda row: len(set(row['Caption']).intersection(row['Hashtags'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "529a0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Time since posted\", \"Likes\", \"Unnamed: 0\", \"S.No\", \"Caption\", \"Hashtags\"], axis=1)\n",
    "y = df[[\"Likes\", \"Time since posted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40bc28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff069699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "cat_feat_to_encode = ['USERNAME']  # Specify the categorical variable to encode\n",
    "username_encode = ce.CatBoostEncoder(cols=cat_feat_to_encode)  # avoids data leakage\n",
    "\n",
    "X_train[\"USERNAME\"] = username_encode.fit_transform(X_train[\"USERNAME\"], y_train[\"Time since posted\"])\n",
    "X_test[\"USERNAME\"] = username_encode.fit_transform(X_test[\"USERNAME\"], y_test[\"Time since posted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "de1bd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ad23cb8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"linear_regression\": LinearRegression(),\n",
    "    \"ridge_regression\": Ridge(random_state=42),\n",
    "    \"lasso\": Lasso(random_state=42),\n",
    "    \"etl\": ExtraTreesRegressor(random_state=42, criterion=\"friedman_mse\", n_estimators=51),\n",
    "    \"rf\": RandomForestRegressor(random_state=42),\n",
    "    \"knn\": KNeighborsRegressor(),    \n",
    "}\n",
    "\n",
    "r2_accuracy = []\n",
    "trained_models_list = []\n",
    "rmse_list = []\n",
    "\n",
    "# looping through dictionary, create model and evaluates it\n",
    "for model in list(models.values()):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2_accuracy.append(r2)\n",
    "    rmse_list.append((rmse))\n",
    "    trained_models_list.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2bbd1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_max_value = max(r2_accuracy)\n",
    "r2_max_index = r2_accuracy.index(r2_max_value)\n",
    "best_model = trained_models_list[r2_max_index]\n",
    "best_model_name = list(models.keys())[r2_max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "593b630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etl: 48.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"{best_model_name}: {round(r2_max_value, 2)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f11da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
