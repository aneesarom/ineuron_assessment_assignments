{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca96bf6d",
   "metadata": {},
   "source": [
    "### Q-3. Imagine you have a dataset where you have different categories of data, Now you need to find the most similar data to the given data by using any 4 different similarity algorithms. Now you have to build a model which can find the most similar data to the given data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b594a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords # To handle stopwords dictionary\n",
    "from nltk.stem import WordNetLemmatizer # lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string #to handle punctuations and special characters\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer # import CountVectorizer to create DTM\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # import to convert DTM to TF-IDF\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3792c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de880004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"News_Category_Dataset_v3.json\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "787b8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.authors = df[\"authors\"].apply(lambda x: x.split(\",\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e661c53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                          37418\n",
       "Lee Moran                                  2955\n",
       "Ron Dicker                                 2219\n",
       "Ed Mazza                                   1591\n",
       "Reuters                                    1577\n",
       "                                          ...  \n",
       "EMMANUEL JARRY AND ROBERT-JAN BARTUNEK        1\n",
       "JIM SALTER & JIM SUHR                         1\n",
       "BRIAN GROSH                                   1\n",
       "Collin Binkley and Errin Haines Whack         1\n",
       "John Giacobbi                                 1\n",
       "Name: authors, Length: 25063, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"authors\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "690374b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f14305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_cols = [\"headline\", \"short_description\"]\n",
    "\n",
    "nlp_pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('tfidf_transformer', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98c502e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map the part of speech tags from NLTK to WordNet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNet lemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def message_text_process(mess):\n",
    "    mess = remove_word_with_digit_and_non_ascii(mess)\n",
    "    # remove punctuation from string\n",
    "    no_punctuation = [char for char in mess if char not in string.punctuation] \n",
    "    no_punctuation = ''.join(no_punctuation) # joining all the characters without punctuation\n",
    "    # remove stop words\n",
    "    words = [word.lower() for word in no_punctuation.split() if word not in stopwords.words('english')]\n",
    "    words = \" \".join(words)\n",
    "    # lemmatize the text\n",
    "    tokens = nltk.word_tokenize(words)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, pos=get_wordnet_pos(token))\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def remove_word_with_digit_and_non_ascii(sentence):\n",
    "    final_sentence = \"\"\n",
    "    sentence = ''.join(c for c in sentence if ord(c) < 128)\n",
    "    for word in sentence:\n",
    "        if not any(char.isdigit() for char in word):\n",
    "            final_sentence += word\n",
    "    return final_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3643c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"date\", \"link\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d94eff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=message_text_process)\n",
    "X_train_bagwords = count_vectorizer.fit_transform(X[\"headline\"])\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_bagwords)\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=message_text_process)\n",
    "X_train_bagwords_2 = count_vectorizer.fit_transform(X[\"short_description\"])\n",
    "X_train_tfidf_2 = tfidf_transformer.fit_transform(X_train_bagwords_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "544853aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209527, 55202)\n",
      "(209527, 77429)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92b36b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 76079)\t0.1761488231929779\n",
      "  (0, 75102)\t0.21493621669984492\n",
      "  (0, 71033)\t0.15590526641403696\n",
      "  (0, 59738)\t0.13914186762046307\n",
      "  (0, 53657)\t0.3124966409027681\n",
      "  (0, 49261)\t0.23519451605803626\n",
      "  (0, 46457)\t0.1530436814592696\n",
      "  (0, 43424)\t0.21760227246131403\n",
      "  (0, 41689)\t0.2848370302496648\n",
      "  (0, 29709)\t0.19954409781496518\n",
      "  (0, 22675)\t0.22923295575386676\n",
      "  (0, 22215)\t0.24965763800549143\n",
      "  (0, 19784)\t0.24027084079289215\n",
      "  (0, 18899)\t0.37020362588712913\n",
      "  (0, 16998)\t0.2648519486667227\n",
      "  (0, 8085)\t0.39320395952163173\n",
      "  (0, 129252)\t0.19958875504935653\n",
      "  (0, 122735)\t0.4332176863429729\n",
      "  (0, 119424)\t0.3004921222703465\n",
      "  (0, 113109)\t0.20360321673985385\n",
      "  (0, 112432)\t0.5053056399828995\n",
      "  (0, 108785)\t0.246813544200962\n",
      "  (0, 95511)\t0.13526587305463647\n",
      "  (0, 88257)\t0.30398160318286566\n",
      "  (0, 83302)\t0.4070425623309862\n",
      "  :\t:\n",
      "  (209525, 80083)\t0.2712330225019758\n",
      "  (209525, 78471)\t0.4788785832542771\n",
      "  (209526, 70709)\t0.35378991830181156\n",
      "  (209526, 69520)\t0.3251863314405932\n",
      "  (209526, 68199)\t0.08975515900201514\n",
      "  (209526, 67615)\t0.30449323861381705\n",
      "  (209526, 58595)\t0.2637563220044615\n",
      "  (209526, 49352)\t0.27456666342819125\n",
      "  (209526, 46756)\t0.18091171097632106\n",
      "  (209526, 39815)\t0.1861776821112845\n",
      "  (209526, 25234)\t0.20356164528101534\n",
      "  (209526, 23931)\t0.3575591618821358\n",
      "  (209526, 20857)\t0.30158199771157906\n",
      "  (209526, 13454)\t0.23529852848381408\n",
      "  (209526, 11181)\t0.20710000497576503\n",
      "  (209526, 1806)\t0.31221080616850916\n",
      "  (209526, 126985)\t0.10945512537653397\n",
      "  (209526, 125994)\t0.3955812729075064\n",
      "  (209526, 119118)\t0.3043627536800787\n",
      "  (209526, 106688)\t0.31756763783552405\n",
      "  (209526, 106112)\t0.28440868169141925\n",
      "  (209526, 100371)\t0.3527080667169247\n",
      "  (209526, 100213)\t0.4584534223885754\n",
      "  (209526, 91939)\t0.4333532091842738\n",
      "  (209526, 78112)\t0.18650286503638813\n"
     ]
    }
   ],
   "source": [
    "# Combine the sparse matrices\n",
    "train_arr = sparse.hstack((X_train_tfidf_2, X_train_tfidf))\n",
    "\n",
    "print(train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c6869ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categories\n",
    "encoder = LabelEncoder()\n",
    "X[\"category\"] = encoder.fit_transform(X[\"category\"])\n",
    "X[\"authors\"] = encoder.fit_transform(X[\"authors\"])\n",
    "\n",
    "train_arr = sparse.hstack((train_arr, np.array(X[\"authors\"])[:, None], np.array(X[\"category\"])[:, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4b9c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aneesaro/miniforge3/envs/pyc/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal_Clusters: 7\n"
     ]
    }
   ],
   "source": [
    "wcss=[]\n",
    "for k in range(2,40):\n",
    "    kmean=KMeans(n_clusters=k,init=\"k-means++\")\n",
    "    kmean.fit(train_arr)\n",
    "    wcss.append(kmean.inertia_)\n",
    "\n",
    "k = KneeLocator(range(2,40), wcss, curve='convex', direction='decreasing')\n",
    "optimal_clusters = k.elbow\n",
    "\n",
    "print(f\"Optimal_Clusters: {optimal_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f9148f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=7, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=7, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=7, random_state=42)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit the K-Means model\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, init=\"k-means++\", random_state=42)  # Specify the number of clusters\n",
    "kmeans.fit(train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcd687fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training silhouette score is: 0.6189240054929624\n"
     ]
    }
   ],
   "source": [
    "silhouette_avg = silhouette_score(train_arr, kmeans.labels_)\n",
    "print(\"The training silhouette score is:\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "184cdf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clusters\"] = kmeans.labels_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
