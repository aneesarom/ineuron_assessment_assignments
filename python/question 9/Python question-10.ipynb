{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0eae03",
   "metadata": {},
   "source": [
    "# Question 10 -\n",
    "```\n",
    "Write a program to count the number of verbs, nouns, pronouns, and adjectives in a given particular phrase or paragraph, and return their respective count as a dictionary.\n",
    "Note -\n",
    "1. Write code comments wherever required for code\n",
    "2. You have to write at least 2 additional test cases in which your program will run successfully and provide an explanation for the same.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2d4d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords # To handle stopwords dictionary\n",
    "from nltk.stem import WordNetLemmatizer # lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string #to handle punctuations and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ee6d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aneesaro/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d74480",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928ef881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map parts of speech tag using by WordNet lemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # returns noun if none of the above matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b8f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since pronouns are not supported by wordnet, added as list\n",
    "\n",
    "pronouns_list = ['I', 'YOU', 'HE', 'SHE', 'IT', 'WE', 'THEY', 'ME', 'YOU', 'HIM', 'HER', 'US', 'THEM', 'MYSELF', 'YOURSELF', 'HIMSELF', 'HERSELF', 'ITSELF', 'OURSELVES', 'YOURSELVES', 'THEMSELVES', 'MINE', 'YOURS', 'HIS', 'HERS', 'ITS', 'OURS', 'THEIRS', 'MY', 'YOUR', 'OUR', 'THEIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ebf0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_word_with_digit_and_non_ascii(sentence):\n",
    "    final_sentence = \"\"\n",
    "    sentence = ''.join(c for c in sentence if ord(c) < 128)\n",
    "    for word in sentence:\n",
    "        if not any(char.isdigit() for char in word):\n",
    "            final_sentence += word\n",
    "    return final_sentence\n",
    "\n",
    "def message_text_process(mess):\n",
    "    mess = remove_word_with_digit_and_non_ascii(mess)\n",
    "    # remove punctuation from string\n",
    "    no_punctuation = [char for char in mess if char not in string.punctuation] \n",
    "    no_punctuation = ''.join(no_punctuation) # joining all the characters without punctuation\n",
    "    # remove stop words\n",
    "    words = [word.lower() for word in no_punctuation.split() if word not in stopwords.words('english')]\n",
    "    words = \" \".join(words)\n",
    "    # lemmatize the text\n",
    "    tokens = nltk.word_tokenize(words)\n",
    "    lemmas = {}\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, pos=get_wordnet_pos(token))\n",
    "        pos_type = nltk.pos_tag([token])[0][1]\n",
    "        if token.upper() in pronouns_list:\n",
    "            lemmas[token] = {\"P\":token}\n",
    "        else:\n",
    "            lemmas[token] = {pos_type[0]:token}\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "090250e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_count(message):\n",
    "    pos_dict = message_text_process(message)\n",
    "\n",
    "    verb_count = 0\n",
    "    noun_count = 0\n",
    "    adjective_count = 0\n",
    "    pronoun_count = 0\n",
    "\n",
    "    for key, value in pos_dict.items():\n",
    "        if list(value.keys())[0] == \"N\":\n",
    "            noun_count += 1\n",
    "        elif list(value.keys())[0] == \"V\":\n",
    "            verb_count += 1\n",
    "        elif list(value.keys())[0] == \"J\":\n",
    "            adjective_count += 1\n",
    "        elif list(value.keys())[0] == \"P\":\n",
    "            pronoun_count += 1\n",
    "    \n",
    "    dic = {\n",
    "    \"nouns\": f\"{verb_count}\",\n",
    "    \"pronouns\": f\"{noun_count}\",\n",
    "    \"verbs\": f\"{pronoun_count}\",\n",
    "    \"adjectives\": f\"{adjective_count}\"\n",
    "    }\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2d0f416",
   "metadata": {},
   "source": [
    "She quickly ran to the store to buy some fresh fruits.\n",
    "They are discussing the interesting book in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a2139af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nouns': '1', 'pronouns': '3', 'verbs': '1', 'adjectives': '1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count(\"She quickly ran to the store to buy some fresh fruits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c1b2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nouns': '2', 'pronouns': '2', 'verbs': '1', 'adjectives': '0'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count(\"They are discussing the interesting book in the library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb793d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
